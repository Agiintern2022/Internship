{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Input, Conv2D, ReLU, BatchNormalization, Flatten, Dense, Reshape, Conv2DTranspose, Activation, Lambda\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MeanSquaredError\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "class VAE:\n",
    "    def __init__(self,\n",
    "                 input_shape,\n",
    "                 conv_filters,\n",
    "                 conv_kernels,\n",
    "                 conv_strides,\n",
    "                 latent_space_dim):\n",
    "        self.input_shape = input_shape   # ex : [28,28,1] For image\n",
    "        self.conv_filters = conv_filters # ex : [2, 4, 8]\n",
    "        self.conv_kernels = conv_kernels # ex : [3, 5, 3]\n",
    "        self.conv_strides = conv_strides # ex : [1, 2, 2]\n",
    "        self.latent_space_dim = latent_space_dim # ex : 2\n",
    "        self.reconstruction_loss_weight = 1000\n",
    "\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.model = None\n",
    "\n",
    "        self._num_conv_layers = len(conv_filters)\n",
    "        self._shape_before_bottleneck = None\n",
    "        self._model_input = None\n",
    "\n",
    "        self._build()\n",
    "\n",
    "    def _build(self):\n",
    "        self._build_encoder()     #Calling _build_encoder Function\n",
    "        self._build_decoder()     #Calling _build_decoder Function \n",
    "        self._build_autoencoder() #Calling _build_autoencoder Function\n",
    "    \n",
    "        \n",
    "    #ENCODER===========================================================================================ENCODER#\n",
    "    #ENCODER===========================================================================================ENCODER#\n",
    "    def _build_encoder(self):\n",
    "        encoder_input = self._add_encoder_input()          # Encoder Helper Function - 1\n",
    "        conv_layers = self._add_conv_layers(encoder_input) # Encoder Helper Function - 2\n",
    "        bottleneck = self._add_bottleneck(conv_layers)     # Encoder Helper Function - 3\n",
    "        self._model_input = encoder_input\n",
    "        self.encoder = Model(encoder_input, bottleneck, name=\"encoder\")\n",
    "\n",
    "    # Encoder Helper Function - 1\n",
    "    def _add_encoder_input(self):\n",
    "        return Input(shape=self.input_shape, name=\"encoder_input\")\n",
    "    \n",
    "    # Encoder Helper Function - 2\n",
    "    def _add_conv_layers(self, encoder_input):\n",
    "        \"\"\"Create all convolutional blocks in encoder.\"\"\"\n",
    "        x = encoder_input\n",
    "        for layer_index in range(self._num_conv_layers):\n",
    "            x = self._add_conv_layer(layer_index, x)    #Call below function to add layers\n",
    "        return x\n",
    "\n",
    "    def _add_conv_layer(self, layer_index, x):\n",
    "        layer_number = layer_index + 1\n",
    "        conv_layer = Conv2D(\n",
    "            filters=self.conv_filters[layer_index],\n",
    "            kernel_size=self.conv_kernels[layer_index],\n",
    "            strides=self.conv_strides[layer_index],\n",
    "            padding=\"same\",\n",
    "            name=f\"encoder_conv_layer_{layer_number}\"\n",
    "        )\n",
    "        x = conv_layer(x)\n",
    "        x = ReLU(name=f\"encoder_relu_{layer_number}\")(x)\n",
    "        x = BatchNormalization(name=f\"encoder_bn_{layer_number}\")(x)\n",
    "        return x\n",
    "    \n",
    "    # Encoder Helper Function - 3\n",
    "    def _add_bottleneck(self, x):\n",
    "        \"\"\"Flatten data and add bottleneck with Guassian sampling (Dense layer).\"\"\"\n",
    "        self._shape_before_bottleneck = K.int_shape(x)[1:]\n",
    "        x = Flatten()(x)\n",
    "        self.mu = Dense(self.latent_space_dim, name=\"mu\")(x)\n",
    "        self.log_variance = Dense(self.latent_space_dim, name=\"log_variance\")(x)\n",
    "\n",
    "        def sample_point_from_normal_distribution(args):\n",
    "            mu, log_variance = args\n",
    "            epsilon = K.random_normal(shape=K.shape(self.mu), mean=0., stddev=1.)\n",
    "            sampled_point = mu + K.exp(log_variance / 2) * epsilon\n",
    "            return sampled_point\n",
    "\n",
    "        x = Lambda(sample_point_from_normal_distribution, name=\"encoder_output\")([self.mu, self.log_variance])\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    #DECODER===========================================================================================DECODER#\n",
    "    #DECODER===========================================================================================DECODER#\n",
    "    def _build_decoder(self):\n",
    "        decoder_input = self._add_decoder_input()                              # Decoder Helper Function - 1\n",
    "        dense_layer = self._add_dense_layer(decoder_input)                     # Decoder Helper Function - 2\n",
    "        reshape_layer = self._add_reshape_layer(dense_layer)                   # Decoder Helper Function - 3\n",
    "        conv_transpose_layers = self._add_conv_transpose_layers(reshape_layer) # Decoder Helper Function - 4\n",
    "        decoder_output = self._add_decoder_output(conv_transpose_layers)       # Decoder Helper Function - 5\n",
    "        self.decoder = Model(decoder_input, decoder_output, name=\"decoder\")\n",
    "    \n",
    "    # Decoder Helper Function - 1\n",
    "    def _add_decoder_input(self):\n",
    "        return Input(shape=self.latent_space_dim, name=\"decoder_input\")\n",
    "    \n",
    "    # Decoder Helper Function - 2\n",
    "    def _add_dense_layer(self, decoder_input):\n",
    "        num_neurons = np.prod(self._shape_before_bottleneck) # [1, 2, 4] -> 8\n",
    "        dense_layer = Dense(num_neurons, name=\"decoder_dense\")(decoder_input)\n",
    "        return dense_layer\n",
    "    \n",
    "    # Decoder Helper Function - 3\n",
    "    def _add_reshape_layer(self, dense_layer):\n",
    "        return Reshape(self._shape_before_bottleneck)(dense_layer)\n",
    "    \n",
    "    # Decoder Helper Function - 4\n",
    "    def _add_conv_transpose_layers(self, x):\n",
    "        \"\"\"Add conv transpose blocks.\"\"\"\n",
    "        # loop through all the conv layers in reverse order and stop at the\n",
    "        # first layer\n",
    "        for layer_index in reversed(range(1, self._num_conv_layers)):\n",
    "            x = self._add_conv_transpose_layer(layer_index, x)\n",
    "        return x\n",
    "\n",
    "    def _add_conv_transpose_layer(self, layer_index, x):\n",
    "        layer_num = self._num_conv_layers - layer_index\n",
    "        conv_transpose_layer = Conv2DTranspose(\n",
    "            filters=self.conv_filters[layer_index],\n",
    "            kernel_size=self.conv_kernels[layer_index],\n",
    "            strides=self.conv_strides[layer_index],\n",
    "            padding=\"same\",\n",
    "            name=f\"decoder_conv_transpose_layer_{layer_num}\"\n",
    "        )\n",
    "        x = conv_transpose_layer(x)\n",
    "        x = ReLU(name=f\"decoder_relu_{layer_num}\")(x)\n",
    "        x = BatchNormalization(name=f\"decoder_bn_{layer_num}\")(x)\n",
    "        return x\n",
    "    \n",
    "    # Decoder Helper Function - 5\n",
    "    def _add_decoder_output(self, x):\n",
    "        conv_transpose_layer = Conv2DTranspose(\n",
    "            filters=1,\n",
    "            kernel_size=self.conv_kernels[0],\n",
    "            strides=self.conv_strides[0],\n",
    "            padding=\"same\",\n",
    "            name=f\"decoder_conv_transpose_layer_{self._num_conv_layers}\"\n",
    "        )\n",
    "        x = conv_transpose_layer(x)\n",
    "        output_layer = Activation(\"sigmoid\", name=\"sigmoid_layer\")(x)\n",
    "        return output_layer\n",
    "\n",
    "    \n",
    "    #BUILDING MODEL FROM ENCODER AND DECODER#==================================================================#\n",
    "    def _build_autoencoder(self):\n",
    "        model_input = self._model_input\n",
    "        model_output = self.decoder(self.encoder(model_input))\n",
    "        self.model = Model(model_input, model_output, name=\"autoencoder\")\n",
    "    \n",
    "    #GENERATING SUMMARY#======================================================================================#\n",
    "    def summary(self):\n",
    "        self.encoder.summary()\n",
    "        self.decoder.summary()\n",
    "        self.model.summary()\n",
    "        \n",
    "\n",
    "    #COMPLILE AND TRAINING MODEL#============================================================================#\n",
    "    def compile(self, learning_rate=0.0001):\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        self.model.compile(optimizer=optimizer,\n",
    "                           loss=self._calculate_combined_loss,\n",
    "                           metrics=[self._calculate_reconstruction_loss,\n",
    "                                    self._calculate_kl_loss])\n",
    "        \n",
    "    def train(self, x_train, batch_size, num_epochs):\n",
    "        self.model.fit(x_train,\n",
    "                       x_train,\n",
    "                       batch_size=batch_size,\n",
    "                       epochs=num_epochs,\n",
    "                       shuffle=True)\n",
    "\n",
    "    #LOSS FUNCTIONS#========================================================================================#\n",
    "    def _calculate_combined_loss(self, y_target, y_predicted):\n",
    "        reconstruction_loss = self._calculate_reconstruction_loss(y_target, y_predicted)\n",
    "        kl_loss = self._calculate_kl_loss(y_target, y_predicted)\n",
    "        combined_loss = self.reconstruction_loss_weight * reconstruction_loss + kl_loss\n",
    "        return combined_loss\n",
    "\n",
    "    def _calculate_reconstruction_loss(self, y_target, y_predicted):\n",
    "        error = y_target - y_predicted\n",
    "        reconstruction_loss = K.mean(K.square(error), axis=[1, 2, 3])\n",
    "        return reconstruction_loss\n",
    "\n",
    "    def _calculate_kl_loss(self, y_target, y_predicted):\n",
    "        kl_loss = -0.5 * K.sum(1 + self.log_variance - K.square(self.mu) - K.exp(self.log_variance), axis=1)\n",
    "        return kl_loss\n",
    "    \n",
    "        \n",
    "    #RECONSTRUCTING INPUT#===============================================================================#\n",
    "    def reconstruct(self, input_data):\n",
    "        latent_representations = self.encoder.predict(input_data)\n",
    "        reconstructed_data = self.decoder.predict(latent_representations)\n",
    "        return reconstructed_data,latent_representations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import nltk\n",
    "# import pandas as pd\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=pd.read_csv(r'F:\\4 - DESKTOP\\archive datasets\\imdb_movie.csv')\n",
    "# print(data.shape)\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=data.iloc[:1000,:]\n",
    "# data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import PorterStemmer\n",
    "# ps=PorterStemmer()\n",
    "\n",
    "# clean_data=[]\n",
    "# for text in data['review']:\n",
    "#     text=text.lower()\n",
    "#     text=re.sub('[^A-Za-z]',' ',text)\n",
    "#     text = text.split()\n",
    "#     text=[ps.stem(word) for word in text if word not in set(stopwords.words('english'))]\n",
    "#     text=' '.join(text)\n",
    "#     clean_data.append(text)\n",
    "\n",
    "# # clean_data=pd.DataFrame(data_lst)\n",
    "# # data['review']=clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.text import one_hot\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# voc_size=5000\n",
    "# one_hot_repr=[one_hot(word,voc_size) for word in clean_data]\n",
    "# one_hot_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(one_hot_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=pad_sequences(one_hot_repr,padding='pre',maxlen=20)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEARNING_RATE = 0.0005\n",
    "# BATCH_SIZE = 32\n",
    "# EPOCHS = 10\n",
    "# autoencoder = train(data, LEARNING_RATE, BATCH_SIZE, EPOCHS, (None,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense,Dropout,LSTM\n",
    "# from keras.layers.embeddings import Embedding\n",
    "# embedding_vector_features=40\n",
    "\n",
    "# model=Sequential()\n",
    "# model.add(Embedding(voc_size,embedding_vector_features,input_length=20))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(LSTM(100))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(1,activation='sigmoid'))\n",
    "# model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "# model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=10,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
